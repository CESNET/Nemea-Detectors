#!/usr/bin/env python3

import xml.etree.ElementTree as ET
import requests
import logging
import re
import sched
import time
import os
import subprocess
import argparse
from contextlib import suppress

config_file = '@BLACKLISTFILTERDIR@/bl_downloader_config.xml'

SECONDS_IN_MINUTE = 60

cs = logging.StreamHandler()
formatter = logging.Formatter('[%(asctime)s] - %(levelname)s - %(message)s')
cs.setFormatter(formatter)
logger = logging.getLogger(__name__)
logger.addHandler(cs)

ip_regex = re.compile('\\b((2(5[0-5]|[0-4][0-9])|[01]?[0-9][0-9]?)\.){3}(2(5[0-5]|[0-4][0-9])|[01]?[0-9][0-9]?)((/(3[012]|[12]?[0-9]))?)\\b')

# Just a simple regex to eliminate commentaries
url_regex = re.compile('^[^#/].*\..+')

# A heterogenic list with all types of blacklists
blacklists = []

# Git repo used for versioning blacklists
repo_path = None


# Sorting comparator, splits the IP in format "A.B.C.D(/X),Y,Z"
# into tuple of IP (A, B, C, D), which is comparable by python (numerically)
def split_ip(ip):
    # Extract only IP, without the prefix and indexes
    ip = ip.split('/')[0] if '/' in ip else ip.split(',')[0]

    try:
        tuple_ip = tuple(int(part) for part in ip.split('.'))
    except ValueError as e:
        logger.warning('Couldnt sort this IP addr: {}'.format(ip))
        logger.warning(e)
        tuple_ip = (0, 0, 0, 0)

    """Split a IP address given as string into a 4-tuple of integers."""
    return tuple_ip


class GeneralConfig:
    def __init__(self, general_config):
        for elem in general_config:
            with suppress(ValueError):
                setattr(self, elem.attrib['name'], int(elem.text))


class Blacklist:
    def __init__(self, bl):
        self.entities = []
        self.last_download = 1

        # Generate variables from the XML config file
        for element in bl:
            setattr(self, element.attrib['name'], element.text)

    def __str__(self):
        return str(self.__dict__)

    def write_to_repo(self):
        if isinstance(self, IPBlacklist):
            type_dir = 'ip'
        elif isinstance(self, URLBlacklist):
            type_dir = 'url'

        bl_file = '{}/{}/{}'.format(repo_path, type_dir, self.name)
        with open(bl_file, 'w') as f:
            f.write('\n'.join(self.entities))

    def download_and_update(self):
        updated = False

        try:
            req = requests.get(self.source, timeout=g_conf.socket_timeout)

            if req.status_code == 200:
                data = req.content

                new_entities = self.extract_entities(data)

                if new_entities != self.entities:
                    # Blacklist entities changed since last download
                    self.entities = sorted(new_entities, key=type(self).comparator)
                    if repo_path:
                        self.write_to_repo()
                    self.last_download = time.time()
                    updated = True

                    logger.info('Updated {}: {}'.format(type(self).__name__, self.name))

            else:
                logger.warning('Couldnt fetch blacklist: {}\n'
                               'Status code: {}'.format(self.source, req.status_code))

        except requests.RequestException as e:
            logger.warning('Couldnt fetch blacklist: {}\n'
                           '{}'.format(self.source, e))

        return updated


class IPBlacklist(Blacklist):
    separator = ','
    comparator = split_ip

    def __init__(self, bl):
        super().__init__(bl)

    @staticmethod
    def extract_entities(data):
        extracted = []

        for line in data.decode('utf-8').splitlines():
            match = re.search(ip_regex, line)
            if match:
                extracted.append(match.group(0))

        return extracted


class URLBlacklist(Blacklist):
    separator = '\\'
    comparator = str

    def __init__(self, bl):
        super().__init__(bl)

    @staticmethod
    def extract_entities(data):
        extracted = []

        for line in data.decode('utf-8').splitlines():
            match = re.search(url_regex, line)
            if match:
                url = match.group(0)
                url = url.replace('https://', '', 1)
                url = url.replace('http://', '', 1)
                url = url.replace('www.', '', 1)
                url = url.lower()
                while url[-1] == '/':
                    url = url[:-1]
                # TODO: Maybe normalize also?
                extracted.append(url)

        return extracted


def create_detector_file(bl_type: Blacklist):
    all_entities = dict()

    # Enrich the entities with blacklist index (bitfield way), merge the indexes if the same entity
    # found on more blacklists
    for bl in blacklists:
        if isinstance(bl, bl_type):
            bl_idx = 2 ** (int(bl.id) - 1)
            for entity in bl.entities:
                if entity in all_entities.keys() and all_entities[entity] & bl_idx == 0:
                    all_entities[entity] = all_entities[entity] + bl_idx
                else:
                    all_entities[entity] = bl_idx

    # Create sorted list of entities and their cumulative indexes
    all_entities = sorted(['{entity}{sep}{idx}'.format(entity=entity, idx=idx, sep=bl_type.separator)
                           for entity, idx in all_entities.items()],
                           key=bl_type.comparator)

    os.makedirs(os.path.dirname(bl_type.detector_file), exist_ok=True)
    try:
        with open(bl_type.detector_file, 'w') as f:
            f.write('\n'.join(all_entities))
    except OSError as e:
        logger.critical('Failed to create detector file. {}. Exitting downloader'.format(e))
        exit(1)


def parse_config():
    tree = ET.parse(config_file)

    r = tree.getroot().getchildren()

    general_config = r[0].getchildren()
    detector_files = r[1].getchildren()
    bl_type_array = r[2].getchildren()

    global g_conf
    g_conf = GeneralConfig(general_config)

    for detector_file in detector_files:
        if detector_file.attrib['name'] == "IP":
            IPBlacklist.detector_file = detector_file.text
        elif detector_file.attrib['name'] == "URL":
            URLBlacklist.detector_file = detector_file.text

    for bl_type in bl_type_array:
        type = bl_type.attrib['type']
        for bl in bl_type:
            if type == "IP":
                blacklists.append(IPBlacklist(bl))
            elif type == "URL":
                blacklists.append(URLBlacklist(bl))


def prepare_repo():
    if not os.path.isdir(repo_path + '/.git'):
        ret = subprocess.run(['git', 'init', '{}'.format(repo_path)], check=True, stdout=subprocess.PIPE)
        os.makedirs(repo_path + '/ip', exist_ok=True)
        os.makedirs(repo_path + '/url', exist_ok=True)

        subprocess.run(['git', '--git-dir', '{}'.format(repo_path + '/.git'),
                        'config', 'user.name', 'bl_downloader'],
                       check=True)

        subprocess.run(['git', '--git-dir', '{}'.format(repo_path + '/.git'),
                        'config', 'user.email', 'bl_downloader'],
                       check=True)

        logger.info(ret.stdout.decode().strip('\n'))


def commit_to_repo(bl_type):
    try:
        subprocess.run(['git', '--git-dir', '{}'.format(repo_path + '/.git'),
                        '--work-tree', '{}'.format(repo_path), 'add', '-A'], check=True)

        subprocess.run(['git', '--git-dir', '{}'.format(repo_path + '/.git'),
                        'commit', '--allow-empty', '-m', '{}s updated'.format(bl_type.__name__)],
                       check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

        logger.info('Commited changes to GIT')

    except subprocess.CalledProcessError as e:
        logger.error("Couldnt add/commit to git repo: {}".format(e))


def run(s):
    # schedule next check immediately
    s.enter(g_conf.download_interval * SECONDS_IN_MINUTE, 1, run, (s,))

    for bl_type in [IPBlacklist, URLBlacklist]:
        updated = False

        for bl in blacklists:
            if isinstance(bl, bl_type):
                if bl.last_download and bl.last_download + SECONDS_IN_MINUTE * int(bl.download_interval) < time.time():
                    updated += bl.download_and_update()

        if updated:
            if repo_path:
                commit_to_repo(bl_type)
            create_detector_file(bl_type)
            logger.info('New {} created: {}'.format(bl_type.__name__, bl_type.detector_file))
        else:
            logger.debug('Check for {} updates done, no changes'.format(bl_type.__name__))


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--repo-path',
                        help="If set, blacklists will be saved in this directory and versioned using git. "
                             "Suggested dir: /usr/local/etc/blacklistfilter/blacklist_repo",
                        default=None)

    parser.add_argument('--log-level',
                        help="Logging level value (from standard Logging library, 10 = DEBUG, 20 = INFO etc.)",
                        type=int,
                        default=20)

    args = parser.parse_args()
    repo_path = args.repo_path
    logger.setLevel(args.log_level)

    parse_config()

    if repo_path:
        prepare_repo()

    s = sched.scheduler(time.time, time.sleep)

    s.enter(0, 1, run, (s,))
    s.run()
