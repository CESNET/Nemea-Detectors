#!/usr/bin/env python3

import xml.etree.ElementTree as ET
import requests
import logging
import re
import sched
import time
import os
import subprocess
import argparse
from contextlib import suppress

config_file = '@BLACKLISTFILTERDIR@/bl_downloader_config.xml'

SECONDS_IN_MINUTE = 60

cs = logging.StreamHandler()
formatter = logging.Formatter('[%(asctime)s] - %(levelname)s - %(message)s')
cs.setFormatter(formatter)
logger = logging.getLogger(__name__)
logger.addHandler(cs)

ip_regex = re.compile('\\b((2(5[0-5]|[0-4][0-9])|[01]?[0-9][0-9]?)\.){3}(2(5[0-5]|[0-4][0-9])|[01]?[0-9][0-9]?)((/(3[012]|[12]?[0-9]))?)\\b')

# Just a simple regex to eliminate commentaries
url_regex = re.compile('^[^#/].*\..+')

# FQDN regex for DNS
# taken from: https://github.com/guyhughes/fqdn/blob/develop/fqdn/__init__.py
fqdn_regex = re.compile(r'^((?!-)[-A-Z\d]{1,62}(?<!-)\.)+[A-Z]{1,62}\.?$', re.IGNORECASE)

# A heterogeneous list with all types of blacklists
blacklists = []

# Git repo used for versioning blacklists
repo_path = None


# Sorting comparator, splits the IP in format "A.B.C.D(/X),Y,Z"
# into tuple of IP (A, B, C, D), which is comparable by python (numerically)
def split_ip(ip):
    # Extract only IP, without the prefix and indexes
    ip = ip.split('/')[0] if '/' in ip else ip.split(',')[0]

    try:
        tuple_ip = tuple(int(part) for part in ip.split('.'))
    except ValueError as e:
        logger.warning('Could not sort this IP addr: {}'.format(ip))
        logger.warning(e)
        tuple_ip = (0, 0, 0, 0)

    """Split a IP address given as string into a 4-tuple of integers."""
    return tuple_ip


class GeneralConfig:
    def __init__(self, general_config):
        for elem in general_config:
            with suppress(ValueError):
                setattr(self, elem.attrib['name'], int(elem.text))


class Blacklist:
    def __init__(self, bl):
        self.entities = []
        self.last_download = 1

        # Generate variables from the XML config file
        for element in bl:
            setattr(self, element.attrib['name'], element.text)

    def __str__(self):
        return str(self.__dict__)

    def write_to_repo(self):
        if isinstance(self, IPBlacklist):
            type_dir = 'ip'
        elif isinstance(self, URLandDNSBlacklist):
            type_dir = 'url_dns'

        bl_file = '{}/{}/{}'.format(repo_path, type_dir, self.name)
        with open(bl_file, 'w') as f:
            f.write('\n'.join(self.entities))

    @classmethod
    def process_entities(cls):
        all_entities = dict()

        # Enrich the entities with blacklist index (bitfield way), merge the indexes if the same entity
        # found on more blacklists
        for bl in blacklists:
            if isinstance(bl, cls):
                bl_idx = 2 ** (int(bl.id) - 1)
                for entity in bl.entities:
                    if entity in all_entities.keys() and all_entities[entity] & bl_idx == 0:
                        all_entities[entity] = all_entities[entity] + bl_idx
                    else:
                        all_entities[entity] = bl_idx

        # Create sorted list of entities and their cumulative indexes
        all_entities = sorted(['{entity}{sep}{idx}'.format(entity=entity, idx=idx, sep=cls.separator)
                               for entity, idx in all_entities.items()],
                              key=cls.comparator)

        return all_entities

    def download_and_update(self):
        updated = False

        try:
            req = requests.get(self.source, timeout=g_conf.socket_timeout)

            if req.status_code == 200:
                data = req.content

                new_entities = sorted(self.extract_entities(data), key=type(self).comparator)

                if new_entities != self.entities:
                    # Blacklist entities changed since last download
                    self.entities = new_entities
                    if repo_path:
                        self.write_to_repo()
                    self.last_download = time.time()
                    updated = True

                    logger.info('Updated {}: {}'.format(type(self).__name__, self.name))

            else:
                logger.warning('Could not fetch blacklist: {}\n'
                               'Status code: {}'.format(self.source, req.status_code))

        except requests.RequestException as e:
            logger.warning('Could not fetch blacklist: {}\n'
                           '{}'.format(self.source, e))

        return updated


class IPBlacklist(Blacklist):
    separator = ','
    comparator = split_ip
    ip_detector_file = None

    def __init__(self, bl):
        super().__init__(bl)

    @staticmethod
    def extract_entities(data):
        extracted = []

        for line in data.decode('utf-8').splitlines():
            match = re.search(ip_regex, line)
            if match:
                extracted.append(match.group(0))

        return extracted

    @classmethod
    def create_detector_file(cls):
        entities = cls.process_entities()

        os.makedirs(os.path.dirname(cls.ip_detector_file), exist_ok=True)

        try:
            with open(cls.ip_detector_file, 'w') as f:
                f.write('\n'.join(entities))

            logger.info('New IP detector file created: {}'.format(cls.ip_detector_file))

        except OSError as e:
                logger.critical('Failed to create detector file. {}. Exiting downloader'.format(e))
                exit(1)


class URLandDNSBlacklist(Blacklist):
    separator = '\\'
    comparator = str
    url_detector_file = None
    dns_detector_file = None

    def __init__(self, bl):
        super().__init__(bl)

    @staticmethod
    def extract_entities(data):
        extracted = []

        for line in data.decode('utf-8').splitlines():
            match = re.search(url_regex, line)
            if match:
                url = match.group(0)
                url = url.replace('https://', '', 1)
                url = url.replace('http://', '', 1)
                url = url.replace('www.', '', 1)
                url = url.lower()
                while url[-1] == '/':
                    url = url[:-1]
                # TODO: Maybe normalize also?
                extracted.append(url)

        return extracted

    @classmethod
    def create_detector_file(cls):
        entities = cls.process_entities()

        os.makedirs(os.path.dirname(cls.url_detector_file), exist_ok=True)
        os.makedirs(os.path.dirname(cls.dns_detector_file), exist_ok=True)

        extracted_fqdns = [entity for entity in entities if
                           re.search(fqdn_regex, entity[:entity.find(cls.separator)])]

        try:
            with open(cls.url_detector_file, 'w') as url_f, open(cls.dns_detector_file, 'w') as dns_f:
                url_f.write('\n'.join(entities))
                dns_f.write('\n'.join(extracted_fqdns))

            logger.info('New URL detector file created: {}'.format(cls.url_detector_file))
            logger.info('New DNS detector file created: {}'.format(cls.dns_detector_file))

        except OSError as e:
            logger.critical('Failed to create detector file. {}. Exiting downloader'.format(e))
            exit(1)


def parse_config():
    tree = ET.parse(config_file)

    r = tree.getroot().getchildren()

    general_config = r[0].getchildren()
    detector_files = r[1].getchildren()
    bl_type_array = r[2].getchildren()

    global g_conf
    g_conf = GeneralConfig(general_config)

    IPBlacklist.ip_detector_file = [det_file.text for det_file in detector_files if det_file.attrib['name'] == 'IP'][0]
    URLandDNSBlacklist.url_detector_file = [det_file.text for det_file in detector_files if det_file.attrib['name'] == 'URL'][0]
    URLandDNSBlacklist.dns_detector_file = [det_file.text for det_file in detector_files if det_file.attrib['name'] == 'DNS'][0]

    for bl_type in bl_type_array:
        type = bl_type.attrib['type']
        for bl in bl_type:
            if type == "IP":
                blacklists.append(IPBlacklist(bl))
            elif type == "URL/DNS":
                blacklists.append(URLandDNSBlacklist(bl))


def prepare_repo():
    if not os.path.isdir(repo_path + '/.git'):
        ret = subprocess.check_output(['git', 'init', '{}'.format(repo_path)])

        os.makedirs(repo_path + '/ip', exist_ok=True)
        os.makedirs(repo_path + '/url_dns', exist_ok=True)

        subprocess.check_call(['git', '--git-dir', '{}'.format(repo_path + '/.git'),
                               'config', 'user.name', 'bl_downloader'])

        subprocess.check_call(['git', '--git-dir', '{}'.format(repo_path + '/.git'),
                               'config', 'user.email', 'bl_downloader'], universal_newlines=True)

        logger.info(ret.decode().strip())


def commit_to_repo(bl_type):
    try:
        subprocess.check_call(['git', '--git-dir', '{}'.format(repo_path + '/.git'),
                               '--work-tree', '{}'.format(repo_path), 'add', '-A'],)

        subprocess.check_call(['git', '--git-dir', '{}'.format(repo_path + '/.git'),
                              'commit', '--allow-empty', '-m', '{}s updated'.format(bl_type.__name__)],
                              stdout=subprocess.DEVNULL)

        logger.info('Committed changes to GIT')

    except subprocess.CalledProcessError as e:
        logger.error("Could not add/commit to git repo: {}".format(e))


def run(s):
    # schedule next check immediately
    s.enter(g_conf.download_interval * SECONDS_IN_MINUTE, 1, run, (s,))

    for bl_type in [IPBlacklist, URLandDNSBlacklist]:
        updated = False

        for bl in blacklists:
            if isinstance(bl, bl_type):
                if bl.last_download and bl.last_download + SECONDS_IN_MINUTE * int(bl.download_interval) < time.time():
                    updated += bl.download_and_update()

        if updated:
            if repo_path:
                commit_to_repo(bl_type)
            bl_type.create_detector_file()
        else:
            logger.debug('Check for {} updates done, no changes'.format(bl_type.__name__))


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--repo-path',
                        help="If set, blacklists will be saved in this directory and versioned using git. "
                             "Suggested dir: /usr/local/etc/blacklistfilter/blacklist_repo",
                        default=None)

    parser.add_argument('--log-level',
                        help="Logging level value (from standard Logging library, 10=DEBUG, 20=INFO etc.)",
                        type=int,
                        default=20)

    args = parser.parse_args()
    repo_path = args.repo_path
    logger.setLevel(args.log_level)

    parse_config()

    if repo_path:
        prepare_repo()

    s = sched.scheduler(time.time, time.sleep)

    s.enter(0, 1, run, (s,))
    s.run()
